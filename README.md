# Speculative Decoding for Low-Latency LLM Inference

This repository implements and benchmarks **speculative decoding** for large
language model (LLM) inference with a focus on **latency, throughput, and cost**.

The goal of the project is to study speculative decoding as a **practical
engineering technique**, rather than as a purely theoretical result. Emphasis is
placed on correctness, reproducibility, and deployment-relevant trade-offs.

---

## Background

Speculative decoding accelerates autoregressive generation by using a smaller
**draft** model to propose tokens, which are then verified by a larger
**target** model. Accepted tokens are emitted directly, while rejected tokens
are regenerated by the target model, preserving the exact target distribution.

In this project, speculative decoding is implemented explicitly as a
verification / rejection procedure, and compared against standard
target-only decoding baselines.

---

## Model Setup

A distilled model pair from the same family is used to ensure reasonable
distributional alignment between draft and target:

- **Target model:** `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`
- **Draft model:** `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`

The size gap between the models makes the setup suitable for speculative
decoding, while shared tokenization reduces mismatch effects.

---

## Precision and Quantization

The target model is treated as the reference implementation:

- **Target:** BF16

The draft model is evaluated under two configurations:
- **BF16**, to measure the pure algorithmic effect of speculative decoding
- **INT8 weights**, to approximate a production-oriented deployment scenario

Quantization of the target model is intentionally avoided in the main results to
prevent conflating speculative decoding gains with reduced numerical precision.

---

## Benchmarking

Benchmarks compare:
- standard target-only decoding
- speculative decoding with varying numbers of draft tokens per step

The following metrics are reported:
- latency (ms/token)
- throughput (tokens/sec)
- acceptance rate
- peak GPU memory usage
- estimated cost per generated token

All benchmark parameters (prompt lengths, generation lengths, seeds, hardware)
are recorded to ensure reproducibility.

---

## Hardware

Experiments are run on NVIDIA GPUs, including:
- A100
- H100

Hardware configuration and pricing assumptions are logged for each run.

## Reference

This project is based on the speculative decoding / speculative sampling approach introduced in:

- Leviathan, Y., Kalman, M., & Matias, Y.  
  *Accelerating Large Language Model Decoding with Speculative Sampling* (2022)  
  https://arxiv.org/abs/2211.17192


---

## Repository Structure

```text
src/
  decoding/        baseline and speculative decoding implementations
  bench/           benchmark runner and metric aggregation
  models/          model loading and configuration
configs/           model and benchmark configuration files
scripts/           command-line entrypoints
results/           JSONL logs and summary tables
